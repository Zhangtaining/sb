# Progress Log — Smart Gym System

> Append a new entry here after completing each task or session.
> Format: ## [Date] - Task [ID]: [Title]

---

## 2026-02-28 - T01: Monorepo Scaffold — Directory Structure, Makefile, .env.example

**Summary:** Created the full monorepo skeleton. Initialized uv workspace with shared + 7 service members. Wrote docker-compose.yml for infrastructure (db/redis/minio), Makefile with all standard targets, .env.example with all required env vars, and pyproject.toml stubs for every service.

**Changes:**
- `pyproject.toml` — uv workspace root, dev deps (pytest, httpx)
- `docker-compose.yml` — PostgreSQL+TimescaleDB, Redis, MinIO, all services stubbed with profiles
- `Makefile` — dev-up, dev-down, migrate, test, lint, fmt, register-camera, setup-db targets
- `.env.example` — all env vars documented (DB, Redis, MinIO, Anthropic, JWT, logging)
- `.gitignore` — Python, model weights, Docker, IDE, OS ignores
- `infrastructure/nginx/nginx.conf` — WebSocket-aware proxy config
- `services/*/pyproject.toml` — dependency stubs for all 7 services
- `shared/pyproject.toml` — shared library with SQLAlchemy, Pydantic, Redis, pgvector deps
- All `__init__.py` stubs for every Python package

**Decisions Made:** Nginx moved to `proxy` Docker Compose profile so `dev-up` only starts pure infrastructure (db/redis/minio) without needing the API service to exist. Application services use `cv` profile.

**Context for Next Session:** T02 (shared package — settings, logging, Redis client) is next. Depends on T01 ✓. The `shared/src/gym_shared/` package directories exist but are empty `__init__.py` files only.

**Commit:** 281541d

## 2026-02-28 - T02: Shared Package — Settings, Logging, Redis Client

**Summary:** Implemented the three shared foundation modules. Settings uses Pydantic BaseSettings reading from `.env`. Logging uses structlog supporting both console and JSON formats. Redis client uses asyncio with a shared connection pool.

**Changes:**
- `shared/src/gym_shared/settings.py` — typed Pydantic settings for all env vars
- `shared/src/gym_shared/logging.py` — structlog setup for console/JSON, `get_logger()` helper
- `shared/src/gym_shared/redis_client.py` — async connection pool, `get_redis()` and `get_redis_ctx()`
- `services/*/pyproject.toml` — added `[tool.uv.sources]` gym-shared = workspace
- `pyproject.toml` — excluded perception from workspace (CUDA/Docker only)

**Decisions Made:** PyTorch 2.10 dropped macOS x86_64 support, so the perception service was removed from the local uv workspace. It will only be built/run via Docker. All other services install cleanly.

**Context for Next Session:** T03 (SQLAlchemy ORM models) is next. Depends on T02 ✓. Files go in `shared/src/gym_shared/db/models.py`. Need pgvector `Vector` type from `pgvector.sqlalchemy`.

**Commit:** 9b9fa68

## 2026-02-28 - T03: Shared Package — SQLAlchemy ORM Models

**Summary:** Implemented all 10 ORM models using SQLAlchemy 2.0 Mapped[T] style. All Phase 1 tables defined plus Person, Conversation, Message, GymKnowledge, Notification for Phases 2+. pgvector Vector columns on Track, Person, and GymKnowledge.

**Changes:**
- `shared/src/gym_shared/db/models.py` — all 10 models with relationships and FK constraints

**Decisions Made:** RepEvent and PoseFrame use composite PKs (time + FK) since TimescaleDB requires the partition column in the PK. Hypertable conversion is deferred to the Alembic migration.

**Context for Next Session:** T04 (Alembic migrations) and T05 (docker-compose) can now be worked on. T04 depends on T03 ✓ and T05 ✓. T05 is already complete from T01.

**Commit:** dc1c8f8

## 2026-02-28 - T04: Alembic Migrations + DB Session Factory

**Summary:** Set up async Alembic and wrote the initial migration creating all 10 tables. Session factory uses SQLAlchemy async engine with connection pooling. Migration enables pgvector and timescaledb extensions and converts rep_events/pose_frames to hypertables.

**Changes:**
- `shared/src/gym_shared/db/session.py` — async session factory, `get_db()` context manager
- `shared/src/gym_shared/db/migrations/env.py` — async Alembic env
- `shared/src/gym_shared/db/migrations/versions/d60ecb05d003_initial_schema_phase1.py` — full initial schema
- `alembic.ini` — Alembic config pointing to shared migrations directory

**Decisions Made:** Migration is hand-written (not autogenerated) because DB wasn't running locally. Autogenerate will be used for future incremental migrations. `make migrate` runs `alembic upgrade head`.

**Context for Next Session:** T05 was already done in T01 (docker-compose). T06 (Redis Streams event schemas) is next — depends on T02 ✓. All 6 foundation tasks (T01–T06) are nearly done.

**Commit:** 5c6eb77

## 2026-02-28 - T06: Redis Streams Event Schemas

**Summary:** Implemented all Pydantic v2 frozen event schemas and Redis Streams publisher/consumer helpers. All schemas are frozen and round-trip serialize correctly.

**Changes:**
- `shared/src/gym_shared/events/schemas.py` — 6 event schemas + helper types
- `shared/src/gym_shared/events/publisher.py` — publish(), read_group(), ack(), ensure_consumer_group()

**Decisions Made:** `IdentityResolvedEvent` included now (Phase 2 feature) to avoid schema drift later. `frozen=True` on all schemas prevents accidental mutation between services.

**Context for Next Session:** All 6 foundation tasks (T01–T06) are now COMPLETE. Next tasks are T07 (ingestion scaffold) and T12 (perception scaffold) — both are independent and can run in parallel. T07 is the logical next step per dependency order.

**Commit:** 51b6c87

## 2026-02-28 - T07: Ingestion Service Scaffold

**Summary:** Created the ingestion service scaffold: Dockerfile (Python 3.11 slim + ffmpeg + uv), per-service config module (`CameraConfig`, `IngestionConfig`, `build_config()`), and async main entry point that spawns one reader thread + one publisher coroutine per camera.

**Changes:**
- `services/ingestion/Dockerfile` — Python 3.11 slim, ffmpeg libs, uv install, uv sync for ingestion package
- `services/ingestion/src/ingestion/config.py` — `CameraConfig` / `IngestionConfig` dataclasses + `build_config()` factory reading from shared Settings
- `services/ingestion/src/ingestion/main.py` — async entry point; signal handler for SIGTERM/SIGINT; spawns reader+publisher threads per camera

**Decisions Made:** Config is plain dataclasses (not Pydantic) since it's assembled programmatically from the shared Settings singleton. Dockerfile uses `uv sync --project services/ingestion` to leverage uv workspace.

**Context for Next Session:** T08 (camera_reader.py) and T09 (frame_publisher.py) are complete — see entries below.

**Commit:** (see T09)

---

## 2026-02-28 - T08: Ingestion Service — Camera Reader

**Summary:** Implemented `CameraReader` — one thread per camera. Opens RTSP or local file via PyAV. Downsamples to target FPS using frame-step logic. Compresses frames to JPEG using OpenCV. Handles reconnection with exponential backoff (1s → 30s max). Maintains a rolling deque for future clip worker use.

**Changes:**
- `services/ingestion/src/ingestion/camera_reader.py` — `CameraReader` class, `RawFrame` dataclass, `_encode_jpeg()` via OpenCV, reconnection loop
- `services/ingestion/tests/test_camera_reader.py` — 3 unit tests: dataclass, synthetic MP4 smoke test, reconnection mock

**Decisions Made:** JPEG encoding uses OpenCV (`cv2.imencode`) instead of PyAV codec context — simpler and more reliable across platforms. RTSP-specific options (tcp transport, nobuffer) only applied when URL starts with `rtsp://`.

**Context for Next Session:** T09 frame publisher is also complete (see below).

**Commit:** (see T09)

---

## 2026-02-28 - T09: Ingestion Service — Frame Publisher

**Summary:** Implemented `FramePublisher` — async coroutine that drains the CameraReader queue and XADDs `FrameMessage` events to `frames:{camera_id}` Redis Stream with `MAXLEN ~100` trimming cap.

**Changes:**
- `services/ingestion/src/ingestion/frame_publisher.py` — `FramePublisher` async class; uses `loop.run_in_executor` for blocking queue.get; publishes via shared `gym_shared.events.publisher.publish()`

**Decisions Made:** Publisher runs as an async coroutine in a dedicated thread (via `asyncio.run()` in a daemon thread from main.py). This gives each camera its own event loop and avoids cross-camera blocking.

**Context for Next Session:** All T07–T09 complete. Next tasks: T10 (register_camera CLI script), T11 (setup_db script), then T12+ (perception service). T10 and T11 are independent of each other and can both start immediately.

**Commit:** 30fee6a

## 2026-02-28 - T10: Scripts — Register Camera CLI

**Summary:** Implemented `scripts/register_camera.py`. Async argparse CLI that inserts a Camera record into PostgreSQL using the shared ORM. Exits with code 1 and a clear message on duplicate ID.

**Changes:**
- `scripts/register_camera.py` — CLI tool: `--id`, `--rtsp-url`, `--zone`, `--description`

**Decisions Made:** Uses `async with get_db()` directly — no migration tool or raw SQL needed.

**Context for Next Session:** Run with `make dev-up` first to have the DB available.

**Commit:** (see T16)

---

## 2026-02-28 - T11: Scripts — DB Setup Script

**Summary:** Implemented `scripts/setup_db.py`. Thin wrapper around `uv run alembic upgrade head`. Supports `--check` flag to print current revision.

**Changes:**
- `scripts/setup_db.py` — runs migrations via subprocess; replaces manual `make migrate` in CI contexts

**Commit:** (see T16)

---

## 2026-02-28 - T12–T16: Perception Service — Scaffold, YOLO Detector, Tracker, ReID, Pipeline

**Summary:** Full perception service implemented. YOLO11n-pose loaded via ultralytics. ByteTrack tracking integrated via `model.track(persist=True)`. ReID is a zero-vector stub for Phase 1. Pipeline reads from `frames:{camera_id}` Redis Stream, runs detect+track+ReID per frame, publishes `PerceptionEvent` to `perceptions:{camera_id}`.

**Changes:**
- `services/perception/pyproject.toml` — pinned `torch<2.3` (macOS x86_64 compat), `numpy<2.0`, added `lap`; added back to workspace
- `services/perception/src/perception/config.py` — `PerceptionConfig`, `build_config()` with auto device detection
- `services/perception/src/perception/detector.py` — `Detector` class with `detect()` and `track()` methods; normalizes bbox + keypoints to [0,1]
- `services/perception/src/perception/tracker.py` — re-exports `TrackedDetection` (tracking now handled inside `Detector.track()`)
- `services/perception/src/perception/reid_extractor.py` — `ReIDExtractor` stub returning 256-d zero vector
- `services/perception/src/perception/pipeline.py` — `PerceptionPipeline` async class; XREADGROUP → track → publish PerceptionEvent → XACK
- `services/perception/src/perception/main.py` — entry point; shared Detector+ReID instances across camera pipelines
- `scripts/visualize_video.py` — standalone script: YOLO+ByteTrack on an MP4, draws bboxes + skeletons, writes annotated output video
- `services/ingestion/src/ingestion/config.py` — fixed `build_config()` to read `CAMERA_<ID>_RTSP_URL` from env properly via `os.environ`

**Decisions Made:** Merged ByteTrack into `Detector.track()` using ultralytics' `model.track(persist=True)` — cleaner than manually integrating `BYTETracker` which expects a `Results` object in new ultralytics versions. `torch<2.3` is the constraint because 2.2.2 is the last macOS x86_64 wheel; production Docker uses CUDA and will remove the upper bound.

**Context for Next Session:** T10–T16 COMPLETE. Visualizer can run immediately: `uv run python scripts/visualize_video.py input.mp4 output.mp4`. Full pipeline needs `make dev-up` then ingestion + perception services. Next tasks: T17–T24 (exercise service — scaffold, YAML, registry, rep counter, form analyzer, classifier, pipeline). T17 is the logical next step.

**Commit:** ee2b7f5

<!-- Entries will be added here as tasks are completed -->
